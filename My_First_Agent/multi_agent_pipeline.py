"""
Multi-Agent Pipeline for Trustworthy SOTA Tracking
MVP Version: åŸºç¡€ç‰ˆæœ¬ï¼Œå®ç°æ ¸å¿ƒåŠŸèƒ½

æ¶æ„ï¼š
- Agent A (Scanner): æœç´¢è®ºæ–‡ï¼ˆarXiv + Google Scholarï¼‰
- Agent B (Extractor): ä» PDF æå–æ–‡æœ¬å’Œç®€å•è¡¨æ ¼
- Agent C (Normalizer): æŒ‡æ ‡æ ‡å‡†åŒ–å’Œè½¬æ¢
- Agent D (Verifier): å†²çªæ£€æµ‹å’ŒéªŒè¯
"""

import json
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import asyncio

# Agent A: Scanner
class ScannerAgent:
    """Agent A: è´Ÿè´£æœç´¢è®ºæ–‡ï¼ˆä¼˜å…ˆ arXiv + æœ¬åœ°æ¦œå•ç¼“å­˜ï¼›Google Scholar å¯é€‰ï¼‰"""

    def __init__(self, use_scholar: bool = False, scholar_timeout: float = 12.0):
        self.name = "scanner"
        self.use_scholar = use_scholar
        self.scholar_timeout = scholar_timeout

    async def search_arxiv(
        self,
        query: str,
        max_results: int = 10,
        include_terms: Optional[List[str]] = None,
        exclude_terms: Optional[List[str]] = None,
        categories: Optional[List[str]] = None,
        sort_by_recent: bool = False,
        days_window: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """æœç´¢ arXivï¼ˆå¯é…ç½®ï¼šåŒ…å«è¯ã€æ’é™¤è¯ã€åˆ†ç±»ä¸æ’åºï¼‰"""
        try:
            import arxiv, os, asyncio, random
            # é¿å…ä¼ä¸šä»£ç†æ‹¦æˆª arXiv
            for k in ("NO_PROXY", "no_proxy"):
                hosts = os.environ.get(k, "")
                hostset = {h.strip() for h in hosts.split(",") if h.strip()}
                hostset.update({"export.arxiv.org", "arxiv.org"})
                os.environ[k] = ",".join(sorted(hostset))

            client = arxiv.Client()

            # ä¸­æ–‡æŸ¥è¯¢ â†’ è‡ªåŠ¨é™„åŠ è‹±æ–‡åŒä¹‰è¯ï¼ˆä¾‹å¦‚ å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  â†’ MARLï¼‰
            q = (query or "").strip()
            q_lower = q.lower()
            has_cjk = any(ord(ch) > 127 for ch in q)
            extra_terms: List[str] = []
            rl_hint = False
            if has_cjk:
                mapping = {
                    "å¤šæ™ºèƒ½ä½“": ['"multi-agent"', '"multi agent"'],
                    "å¼ºåŒ–å­¦ä¹ ": ['"reinforcement learning"', 'RL'],
                    "åä½œ": ["cooperative", "collaborative"],
                    "åˆ†å¸ƒå¼": ["distributed"],
                    "å»ä¸­å¿ƒåŒ–": ["decentralized"],
                    "åšå¼ˆ": ["game theory", "game"],
                }
                for key, vals in mapping.items():
                    if key in q:
                        extra_terms.extend(vals)
                if ("å¤šæ™ºèƒ½ä½“" in q and "å¼ºåŒ–å­¦ä¹ " in q):
                    extra_terms.extend(['"multi-agent reinforcement learning"', 'MARL'])
                    rl_hint = True
            if "marl" in q_lower:
                rl_hint = True
                extra_terms.extend(['"multi-agent reinforcement learning"', 'MARL'])

            all_include = list(include_terms or []) + extra_terms

            parts = [f"({q})"]
            if all_include:
                inc = " OR ".join([f"ti:{t} OR abs:{t}" for t in all_include])
                parts.append(f"({inc})")
            cats = list(categories or [])
            if rl_hint and not cats:
                cats = ["cs.LG", "cs.AI", "cs.MA"]
            if cats:
                cat = " OR ".join([f"cat:{c}" for c in cats])
                parts.append(f"({cat})")

            merged_query = " AND ".join(parts)

            search = arxiv.Search(
                query=merged_query,
                max_results=max_results,
                sort_by=(arxiv.SortCriterion.SubmittedDate if sort_by_recent else arxiv.SortCriterion.Relevance)
            )

            results: List[Dict[str, Any]] = []
            max_retries = 3
            base_delay = 3.0
            for attempt in range(max_retries):
                try:
                    for paper in client.results(search):
                        title_lower = (paper.title or "").lower()
                        summary_lower = (paper.summary or "").lower()
                        if exclude_terms and any(k.lower() in title_lower or k.lower() in summary_lower for k in exclude_terms):
                            continue

                        # æ—¥æœŸçª—å£è¿‡æ»¤ï¼ˆä¾‹å¦‚æœ€è¿‘ 180/365 å¤©ï¼‰
                        if days_window and paper.published:
                            try:
                                from datetime import datetime, timedelta
                                pub_date = paper.published.date()
                                if datetime.utcnow().date() - pub_date > timedelta(days=days_window):
                                    continue
                            except Exception:
                                pass

                        results.append({
                            "source": "arxiv",
                            "id": paper.get_short_id(),
                            "title": paper.title,
                            "authors": [a.name for a in paper.authors],
                            "summary": paper.summary,
                            "pdf_url": paper.pdf_url,
                            "published": str(paper.published.date()) if paper.published else None,
                            "url": paper.entry_id
                        })
                    break  # æˆåŠŸ
                except Exception as e:
                    msg = str(e).lower()
                    if any(tok in msg for tok in ["429", "rate limit", "proxy", "503", "remote end closed"]):
                        wait = base_delay * (attempt + 1) + random.uniform(0, 0.5)
                        print(f"[arXiv] è¯·æ±‚å—é™æˆ–ä»£ç†å¼‚å¸¸ï¼Œ{wait:.1f}s åé‡è¯• ({attempt+1}/{max_retries})")
                        await asyncio.sleep(wait)
                        continue
                    raise
            # è‹¥æ— ç»“æœä¸”è§¦å‘ RL/MARL è¯­ä¹‰ï¼Œå›é€€ä¸€æ¬¡è‹±æ–‡å¼ºæ£€ç´¢
            if not results and rl_hint:
                fallback_terms = [
                    'ti:"multi-agent reinforcement learning" OR abs:"multi-agent reinforcement learning"',
                    'ti:MARL OR abs:MARL'
                ]
                fb_parts = ["(" + ") AND (".join(fallback_terms) + ")"]
                if cats:
                    fb_parts.append("(" + " OR ".join([f"cat:{c}" for c in cats]) + ")")
                fb_query = " AND ".join(fb_parts)

                search_fb = arxiv.Search(
                    query=fb_query,
                    max_results=max_results,
                    sort_by=(arxiv.SortCriterion.SubmittedDate if sort_by_recent else arxiv.SortCriterion.Relevance)
                )
                try:
                    for paper in client.results(search_fb):
                        results.append({
                            "source": "arxiv",
                            "id": paper.get_short_id(),
                            "title": paper.title,
                            "authors": [a.name for a in paper.authors],
                            "summary": paper.summary,
                            "pdf_url": paper.pdf_url,
                            "published": str(paper.published.date()) if paper.published else None,
                            "url": paper.entry_id
                        })
                except Exception as e:
                    print(f"[arXiv] å›é€€è‹±æ–‡æ£€ç´¢å¤±è´¥: {e}")

            return results
        except Exception as e:
            print(f"[Scanner] arXiv æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_google_scholar(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ Google Scholarï¼ˆå¯é€‰ï¼Œå¢åŠ è¶…æ—¶ä¸å¥å£®æ€§å›é€€ï¼‰"""
        if not self.use_scholar:
            return []
        try:
            import asyncio
            from scholarly import scholarly

            async def _do_search():
                results = []
                try:
                    search_query = scholarly.search_pubs(query)
                    count = 0
                    for pub in search_query:
                        if count >= max_results:
                            break
                        try:
                            pub_filled = scholarly.fill(pub)
                            results.append({
                                "source": "google_scholar",
                                "title": pub_filled.get("bib", {}).get("title", ""),
                                "authors": pub_filled.get("bib", {}).get("author", []),
                                "year": pub_filled.get("bib", {}).get("pub_year", ""),
                                "url": pub_filled.get("pub_url", ""),
                                "pdf_url": pub_filled.get("eprint_url", ""),
                                "citations": pub_filled.get("num_citations", 0)
                            })
                            count += 1
                        except Exception as e:
                            print(f"[Scanner] è·å– Google Scholar è¯¦æƒ…å¤±è´¥: {e}")
                            continue
                except Exception as e:
                    print(f"[Scanner] Google Scholar æœç´¢å†…éƒ¨å¤±è´¥: {e}")
                return results

            try:
                return await asyncio.wait_for(_do_search(), timeout=self.scholar_timeout)
            except asyncio.TimeoutError:
                print(f"[Scanner] Google Scholar æœç´¢è¶…æ—¶ï¼ˆ{self.scholar_timeout}sï¼‰ï¼Œå·²è·³è¿‡å¹¶å›é€€åˆ°å…¶å®ƒæ¥æº")
                return []
        except Exception as e:
            print(f"[Scanner] Google Scholar æœç´¢å¤±è´¥: {e}")
            print("[Scanner] æç¤º: å¦‚æœ scholarly åº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ Google Scholar æœç´¢")
            return []

    def _load_local_leaderboards(self) -> List[Dict[str, Any]]:
        """åŠ è½½æœ¬åœ°æ¦œå•ç¼“å­˜ï¼ˆpapers/*/papers_info.jsonï¼‰"""
        results: List[Dict[str, Any]] = []
        try:
            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "papers"))
            for root, dirs, files in os.walk(base_dir):
                for f in files:
                    if f == "papers_info.json":
                        path = os.path.join(root, f)
                        try:
                            # å…¼å®¹å¤šç§ç¼–ç ä¸ç»“æ„ï¼ˆlist æˆ– dictï¼‰
                            data = None
                            last_err: Optional[Exception] = None
                            for enc in ("utf-8", "utf-8-sig", "gbk", "gb18030", "latin-1"):
                                try:
                                    with open(path, "r", encoding=enc, errors=("strict" if enc != "latin-1" else "ignore")) as fp:
                                        data = json.load(fp)
                                    last_err = None
                                    break
                                except Exception as e:
                                    last_err = e
                                    data = None
                            if data is None and last_err is not None:
                                # å°è¯•äºŒè¿›åˆ¶è¯»å– + å¿½ç•¥é”™è¯¯å†è§£æ
                                try:
                                    with open(path, "rb") as fb:
                                        raw = fb.read()
                                    text = raw.decode("utf-8", errors="ignore")
                                    data = json.loads(text)
                                except Exception:
                                    raise last_err

                            if isinstance(data, list):
                                for item in data:
                                    if isinstance(item, dict):
                                        item["source"] = item.get("source", "leaderboard")
                                        results.append(item)
                            elif isinstance(data, dict):
                                # ä¸€äº›ç¼“å­˜æ˜¯ {id: {...}} å½¢å¼
                                for _, item in data.items():
                                    if isinstance(item, dict):
                                        item["source"] = item.get("source", "leaderboard")
                                        results.append(item)
                        except Exception as e:
                            print(f"[Scanner] è¯»å–æ¦œå•ç¼“å­˜å¤±è´¥: {path} -> {e}")
        except Exception as e:
            print(f"[Scanner] éå†æ¦œå•ç¼“å­˜å¤±è´¥: {e}")
        return results
    
    async def search(self, query: str, max_results_per_source: int = 10) -> Dict[str, Any]:
        """å¤šæºæœç´¢ï¼ˆä¼˜å…ˆç¨³å®šæ¥æºï¼ŒScholar ä¸ºå¯é€‰è¡¥å……ï¼‰"""
        print(f"[Scanner] å¼€å§‹æœç´¢: {query}")

        # å…ˆåŠ è½½æœ¬åœ°æ¦œå•ç¼“å­˜
        leaderboard_results = self._load_local_leaderboards()

        # å¹¶è¡Œæœç´¢ arXiv ä¸ï¼ˆå¯é€‰ï¼‰Scholar
        # é»˜è®¤å¼€å¯æŒ‰æœ€è¿‘æäº¤æ’åºï¼Œå¹¶åº”ç”¨ä¸€å¹´å†…æ—¶é—´çª—
        arxiv_results = await self.search_arxiv(
            query,
            max_results=max_results_per_source,
            sort_by_recent=True,
            days_window=365
        )
        scholar_results = await self.search_google_scholar(query, max_results_per_source)

        # åˆå¹¶ç»“æœï¼ˆç¨³å®šæ¥æºä¼˜å…ˆï¼‰
        total_results = len(leaderboard_results) + len(arxiv_results) + len(scholar_results)
        all_results = {
            "query": query,
            "leaderboard_results": leaderboard_results,
            "arxiv_results": arxiv_results,
            "google_scholar_results": scholar_results,
            "total_results": total_results,
            "timestamp": datetime.utcnow().isoformat(),
            "notes": "ä½¿ç”¨ç¨³å®šæ¥æºä¼˜å…ˆï¼›Scholar ä¸ºå¯é€‰å¹¶å¸¦è¶…æ—¶ä¿æŠ¤"
        }

        print(f"[Scanner] æ‰¾åˆ° {len(leaderboard_results)} ä¸ªæ¦œå•ç¼“å­˜ï¼Œ{len(arxiv_results)} ä¸ª arXiv ç»“æœï¼Œ{len(scholar_results)} ä¸ª Google Scholar ç»“æœ")
        return all_results


# Agent B: Extractor
class ExtractorAgent:
    """Agent B: ä» PDF æå–æ–‡æœ¬å’Œç®€å•è¡¨æ ¼ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼‰"""
    
    def __init__(self, use_vision: bool = False, vision_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ– Extractor
        
        Args:
            use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼º
            vision_model: Vision Model åç§°
        """
        self.name = "extractor"
        self.paper_cache_dir = "papers/extracted"
        os.makedirs(self.paper_cache_dir, exist_ok=True)
        
        self.use_vision = use_vision
        self.vision_extractor = None
        
        if use_vision:
            try:
                from .vision_extractor import VisionExtractor
                self.vision_extractor = VisionExtractor(vision_model)
                print(f"[Extractor] Vision Model å·²å¯ç”¨: {vision_model}")
            except ImportError as e:
                print(f"[Extractor] Vision Extractor å¯¼å…¥å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼")
                self.use_vision = False
    
    def download_pdf(self, pdf_url: str, paper_id: str) -> Optional[str]:
        """ä¸‹è½½ PDFï¼ˆå¢å¼ºï¼šé‡è¯•ã€UA å¤´ã€arXiv é“¾æ¥è§„èŒƒåŒ–ã€è¶…æ—¶ï¼‰"""
        import re
        try:
            import requests
            # è§„èŒƒåŒ– arXiv é“¾æ¥ï¼šå¦‚æœæ˜¯ abs é¡µé¢ï¼Œè½¬ä¸º pdf ä¸‹è½½
            if pdf_url and "arxiv.org" in pdf_url and "/abs/" in pdf_url:
                pdf_url = re.sub(r"/abs/([\w\.-]+)", r"/pdf/\1.pdf", pdf_url)

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
            }
            pdf_path = os.path.join(self.paper_cache_dir, f"{paper_id}.pdf")

            # ç®€å•é‡è¯•æœºåˆ¶
            for attempt in range(3):
                try:
                    response = requests.get(pdf_url, headers=headers, timeout=30)
                    if response.status_code == 200 and response.content:
                        with open(pdf_path, "wb") as f:
                            f.write(response.content)
                        return pdf_path
                    else:
                        print(f"[Extractor] ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}ï¼Œå°è¯• {attempt+1}/3")
                except Exception as e:
                    print(f"[Extractor] ä¸‹è½½å¼‚å¸¸ï¼ˆå°è¯• {attempt+1}/3ï¼‰: {e}")
            # å°è¯•è·Ÿéšé‡å®šå‘ä¸€æ¬¡
            try:
                response = requests.get(pdf_url, headers=headers, timeout=30, allow_redirects=True)
                if response.status_code == 200 and response.content:
                    with open(pdf_path, "wb") as f:
                        f.write(response.content)
                    return pdf_path
            except Exception as e:
                print(f"[Extractor] é‡å®šå‘ä¸‹è½½å¤±è´¥: {e}")
        except Exception as e:
            print(f"[Extractor] ä¸‹è½½ PDF å¤±è´¥ {pdf_url}: {e}")
        return None
    
    def extract_text(self, pdf_path: str) -> str:
        """æå– PDF æ–‡æœ¬ï¼ˆä½¿ç”¨ PyMuPDFï¼‰"""
        try:
            import fitz  # PyMuPDF
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except Exception as e:
            print(f"[Extractor] æ–‡æœ¬æå–å¤±è´¥: {e}")
            # é™çº§åˆ° pdfplumber
            try:
                import pdfplumber
                with pdfplumber.open(pdf_path) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() or ""
                return text
            except Exception as e2:
                print(f"[Extractor] pdfplumber ä¹Ÿå¤±è´¥: {e2}")
                return ""
    
    def extract_tables(self, pdf_path: str) -> List[Dict[str, Any]]:
        """æå–ç®€å•è¡¨æ ¼ï¼ˆä½¿ç”¨ pdfplumberï¼‰"""
        try:
            import pdfplumber
            tables = []
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_tables = page.extract_tables()
                    for table_num, table in enumerate(page_tables):
                        if table and len(table) > 1:  # è‡³å°‘è¦æœ‰è¡¨å¤´å’Œæ•°æ®è¡Œ
                            tables.append({
                                "page": page_num + 1,
                                "table_index": table_num,
                                "data": table,
                                "rows": len(table),
                                "cols": len(table[0]) if table else 0
                            })
            return tables
        except Exception as e:
            print(f"[Extractor] è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def extract_metrics_from_text(self, text: str, context: str = "") -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–æŒ‡æ ‡ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼›å¼ºåŒ–è·Ÿè¸ªé¢†åŸŸå¸¸ç”¨æŒ‡æ ‡ï¼‰"""
        if self.use_vision and self.vision_extractor:
            # ä½¿ç”¨ LLM è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£
            import asyncio
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            asyncio.run,
                            self.vision_extractor.extract_metrics_with_llm(text, context)
                        )
                        metrics = future.result()
                else:
                    metrics = loop.run_until_complete(
                        self.vision_extractor.extract_metrics_with_llm(text, context)
                    )
                
                if metrics:
                    return metrics
            except Exception as e:
                print(f"[Extractor] Vision æŒ‡æ ‡æå–å¤±è´¥ï¼Œé™çº§åˆ°åŸºç¡€æ¨¡å¼: {e}")
        
        # é™çº§åˆ°åŸºç¡€æ­£åˆ™è¡¨è¾¾å¼æå–
        import re
        metrics = []
        
        # å¸¸è§æŒ‡æ ‡æ¨¡å¼ï¼ˆå«è·Ÿè¸ªé¢†åŸŸï¼šAOã€SRã€AUCã€Precisionã€Successï¼‰
        metric_patterns = [
            (r"(?:accuracy|acc)\s*[=:]\s*(\d+\.?\d*)\s*%?", "accuracy"),
            (r"(?:f1[- ]?score|f1)\s*[=:]\s*(\d+\.?\d*)\s*%?", "f1_score"),
            (r"(?:mAP|mean average precision)\s*[=:]\s*(\d+\.?\d*)\s*%?", "mAP"),
            (r"(?:top[- ]?1|top1)\s*[=:]\s*(\d+\.?\d*)\s*%?", "top1_accuracy"),
            (r"(?:top[- ]?5|top5)\s*[=:]\s*(\d+\.?\d*)\s*%?", "top5_accuracy"),
            # Tracking å¸¸ç”¨
            (r"\bAO\b\s*[=:]\s*(\d+\.?\d*)\s*%?", "ao"),
            (r"\bSR\b\s*(?:@?0?\.?5)?\s*[=:]\s*(\d+\.?\d*)\s*%?", "sr"),
            (r"\bAUC\b\s*[=:]\s*(\d+\.?\d*)\s*%?", "auc"),
            (r"(?:precision|prec)\s*[=:]\s*(\d+\.?\d*)\s*%?", "precision"),
            (r"(?:success rate|success)\s*[=:]\s*(\d+\.?\d*)\s*%?", "success_rate"),
        ]
        
        for pattern, metric_name in metric_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                value = float(match.group(1))
                # å¦‚æœæ˜¯ 0-1 èŒƒå›´ï¼Œè½¬æ¢ä¸ºç™¾åˆ†æ¯”
                if value <= 1.0:
                    value = value * 100
                
                metrics.append({
                    "metric": metric_name,
                    "value": value,
                    "unit": "percentage",
                    "context": text[max(0, match.start()-50):match.end()+50]
                })
        
        return metrics
    
    async def extract(self, paper_info: Dict[str, Any]) -> Dict[str, Any]:
        """æå–è®ºæ–‡ä¿¡æ¯"""
        paper_id = paper_info.get("id", "unknown")
        pdf_url = paper_info.get("pdf_url", "")
        published = paper_info.get("published")
        source = paper_info.get("source", "unknown")
        
        if not pdf_url:
            return {
                "paper_id": paper_id,
                "status": "no_pdf",
                "error": "æ²¡æœ‰ PDF URL"
            }
        
        print(f"[Extractor] å¼€å§‹æå–: {paper_id}")
        
        # ä¸‹è½½ PDF
        pdf_path = self.download_pdf(pdf_url, paper_id)
        if not pdf_path:
            return {
                "paper_id": paper_id,
                "status": "download_failed",
                "error": "PDF ä¸‹è½½å¤±è´¥"
            }
        
        # æå–æ–‡æœ¬
        text = self.extract_text(pdf_path)
        
        # æå–è¡¨æ ¼
        tables = self.extract_tables(pdf_path)
        
        # å¦‚æœå¯ç”¨ Vision Modelï¼Œå°è¯•å¢å¼ºè¡¨æ ¼æå–
        if self.use_vision and self.vision_extractor:
            try:
                # æ£€æµ‹è¡¨æ ¼ä½ç½®
                table_locations = self.vision_extractor.detect_tables_in_pdf(pdf_path)
                print(f"[Extractor] æ£€æµ‹åˆ° {len(table_locations)} ä¸ªè¡¨æ ¼ä½ç½®")
                
                # å¯¹æ¯ä¸ªè¡¨æ ¼ä½¿ç”¨ Vision Model ç²¾æå–ï¼ˆå¯é€‰ï¼‰
                # è¿™é‡Œå¯ä»¥æ·»åŠ  Vision Model å¤„ç†é€»è¾‘
            except Exception as e:
                print(f"[Extractor] Vision è¡¨æ ¼å¤„ç†å¤±è´¥: {e}")
        
        # ä»æ–‡æœ¬ä¸­æå–æŒ‡æ ‡ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼‰
        context = f"Title: {paper_info.get('title', '')}\nSummary: {paper_info.get('summary', '')[:500]}"
        metrics = self.extract_metrics_from_text(text, context)
        
        result = {
            "paper_id": paper_id,
            "title": paper_info.get("title", ""),
            "published": published,
            "source": source,
            "status": "success",
            "text_length": len(text),
            "tables_count": len(tables),
            "metrics_count": len(metrics),
            "metrics": metrics,
            "tables": tables[:5],  # åªä¿ç•™å‰5ä¸ªè¡¨æ ¼
            "text_preview": text[:1000]  # æ–‡æœ¬é¢„è§ˆ
        }
        
        print(f"[Extractor] æå–å®Œæˆ: {len(metrics)} ä¸ªæŒ‡æ ‡ï¼Œ{len(tables)} ä¸ªè¡¨æ ¼")
        return result


# Agent C: Normalizer
class NormalizerAgent:
    """Agent C: æŒ‡æ ‡æ ‡å‡†åŒ–å’Œè½¬æ¢"""
    
    def __init__(self):
        self.name = "normalizer"
        
        # æŒ‡æ ‡è½¬æ¢è§„åˆ™ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.metric_conversions = {
            "error_rate": lambda x: 100 - x,  # Error Rate -> Accuracy
            "err": lambda x: 100 - x,
            "error": lambda x: 100 - x,
            "classification_error": lambda x: 100 - x,
            "misclassification_rate": lambda x: 100 - x,
            # æ³¨æ„ï¼šF1 å’Œ Accuracy ä¸èƒ½ç›´æ¥è½¬æ¢ï¼Œéœ€è¦ä¸Šä¸‹æ–‡
        }
        
        # æ•°æ®é›†åˆ«åæ˜ å°„ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.dataset_aliases = {
            "imagenet": ["ILSVRC", "ImageNet-1K", "ImageNet", "ImageNet-1k", "ImageNet1K", "ILSVRC2012"],
            "cifar-10": ["CIFAR-10", "CIFAR10", "cifar10", "CIFAR 10"],
            "cifar-100": ["CIFAR-100", "CIFAR100", "cifar100", "CIFAR 100"],
            "got-10k": ["GOT-10k", "GOT10k", "got10k", "GOT-10K"],
            "lasot": ["LaSOT", "LaSOT", "lasot"],
            "trackingnet": ["TrackingNet", "trackingnet", "Tracking Net"],
            "coco": ["COCO", "coco", "MS COCO", "mscoco"],
            "pascal_voc": ["PASCAL VOC", "Pascal VOC", "VOC", "voc"],
            "cityscapes": ["Cityscapes", "cityscapes", "CityScapes"],
            "otb": ["OTB", "OTB100", "OTB-100", "otb"],
            "uav123": ["UAV123", "uav123"],
            "nfs": ["NFS", "Need for Speed"],
            "tpl": ["TLP", "Tracking-Learning-Prediction"],
        }
        
        # æŒ‡æ ‡ç­‰ä»·å…³ç³»ï¼ˆç”¨äºæ ‡å‡†åŒ–ï¼‰
        self.metric_equivalences = {
            "accuracy": ["acc", "accuracy", "classification accuracy", "top-1 accuracy"],
            "top1_accuracy": ["top-1", "top1", "top 1", "top-1 accuracy", "top1 accuracy"],
            "top5_accuracy": ["top-5", "top5", "top 5", "top-5 accuracy", "top5 accuracy"],
            "f1_score": ["f1", "f1-score", "f1 score", "f1score", "f-measure"],
            "map": ["mAP", "mean average precision", "mean ap", "map"],
            "iou": ["IoU", "iou", "intersection over union", "jaccard index"],
            # Tracking å¸¸è§æŒ‡æ ‡ç­‰ä»·
            "ao": ["ao", "average overlap"],
            "sr": ["sr", "success rate", "success"],
            "auc": ["auc", "area under curve"],
            "precision": ["precision", "prec"],
            "success_rate": ["success rate", "success"],
        }
        
        # æŒ‡æ ‡æ ‡å‡†åŒ–åç§°ï¼ˆä½¿ç”¨ç­‰ä»·å…³ç³»ï¼‰
        self.metric_standard_names = {}
        for standard_name, variants in self.metric_equivalences.items():
            self.metric_standard_names[standard_name] = variants
    
    def normalize_metric_name(self, metric_name: str) -> str:
        """æ ‡å‡†åŒ–æŒ‡æ ‡åç§°"""
        metric_lower = metric_name.lower().strip()
        for standard_name, variants in self.metric_standard_names.items():
            if metric_lower in variants:
                return standard_name
        return metric_name.lower()
    
    def normalize_dataset_name(self, dataset_name: str) -> str:
        """æ ‡å‡†åŒ–æ•°æ®é›†åç§°"""
        dataset_lower = dataset_name.lower().strip()
        for standard_name, aliases in self.dataset_aliases.items():
            if dataset_lower in aliases or dataset_lower == standard_name:
                return standard_name
        return dataset_name
    
    def normalize_value(self, value: float, unit: str) -> Tuple[float, str]:
        """æ ‡å‡†åŒ–æ•°å€¼å’Œå•ä½"""
        # ç»Ÿä¸€è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        if unit in ["decimal", "ratio", "fraction"]:
            return value * 100, "percentage"
        elif unit == "percentage":
            return value, "percentage"
        else:
            # é»˜è®¤å‡è®¾æ˜¯ç™¾åˆ†æ¯”
            if 0 <= value <= 1:
                return value * 100, "percentage"
            return value, "percentage"
    
    def convert_metric(self, metric_name: str, value: float) -> Optional[float]:
        """è½¬æ¢æŒ‡æ ‡ï¼ˆå¦‚ Error Rate -> Accuracyï¼‰"""
        metric_lower = metric_name.lower()
        if metric_lower in self.metric_conversions:
            return self.metric_conversions[metric_lower](value)
        return None
    
    async def normalize(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æå–çš„æ•°æ®"""
        print(f"[Normalizer] å¼€å§‹æ ‡å‡†åŒ–: {extracted_data.get('paper_id', 'unknown')}")
        
        normalized_metrics = []
        
        for metric in extracted_data.get("metrics", []):
            metric_name = metric.get("metric", "")
            value = metric.get("value", 0)
            unit = metric.get("unit", "percentage")
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡åç§°
            normalized_name = self.normalize_metric_name(metric_name)
            
            # æ ‡å‡†åŒ–æ•°å€¼
            normalized_value, normalized_unit = self.normalize_value(value, unit)
            
            # å°è¯•è½¬æ¢ï¼ˆå¦‚ Error Rate -> Accuracyï¼‰
            converted_value = self.convert_metric(metric_name, normalized_value)
            if converted_value is not None:
                normalized_metrics.append({
                    "original_metric": metric_name,
                    "normalized_metric": "accuracy",  # Error Rate è½¬æ¢ä¸º Accuracy
                    "original_value": normalized_value,
                    "normalized_value": converted_value,
                    "unit": normalized_unit,
                    "converted": True,
                    "context": metric.get("context", "")
                })
            else:
                normalized_metrics.append({
                    "original_metric": metric_name,
                    "normalized_metric": normalized_name,
                    "original_value": value,
                    "normalized_value": normalized_value,
                    "unit": normalized_unit,
                    "converted": False,
                    "context": metric.get("context", "")
                })
        
        result = {
            "paper_id": extracted_data.get("paper_id", ""),
            "title": extracted_data.get("title", ""),
            "normalized_metrics": normalized_metrics,
            "metrics_count": len(normalized_metrics),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"[Normalizer] æ ‡å‡†åŒ–å®Œæˆ: {len(normalized_metrics)} ä¸ªæŒ‡æ ‡")
        return result


# Agent D: Verifier
class VerifierAgent:
    """Agent D: å†²çªæ£€æµ‹å’ŒéªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self, conflict_threshold: float = 1.0):
        """
        åˆå§‹åŒ– Verifier
        
        Args:
            conflict_threshold: å†²çªé˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œå·®å¼‚è¶…è¿‡æ­¤å€¼è§†ä¸ºå†²çª
        """
        self.name = "verifier"
        self.conflict_threshold = conflict_threshold
        
        # æ¥æºå¯ä¿¡åº¦æƒé‡
        self.source_weights = {
            "arxiv": 1.0,  # arXiv å®˜æ–¹å‘å¸ƒï¼Œå¯ä¿¡åº¦é«˜
            "google_scholar": 0.8,  # Google Scholar èšåˆï¼Œå¯ä¿¡åº¦ä¸­ç­‰
            "paper_pdf": 0.9,  # ç›´æ¥ä»è®ºæ–‡ PDF æå–ï¼Œå¯ä¿¡åº¦é«˜
            "web": 0.6,  # ç½‘é¡µæ¥æºï¼Œå¯ä¿¡åº¦è¾ƒä½
        }
        
        # æ—¶é—´æ–°é²œåº¦æƒé‡ï¼ˆè¶Šæ–°è¶Šå¯ä¿¡ï¼‰
        self.time_decay_factor = 0.1  # æ¯å¹´è¡°å‡ 10%
    
    def calculate_confidence_score(self, paper_info: Dict[str, Any], metric_info: Dict[str, Any]) -> float:
        """
        è®¡ç®—å•ä¸ªæŒ‡æ ‡çš„ç½®ä¿¡åº¦è¯„åˆ†
        
        Args:
            paper_info: è®ºæ–‡ä¿¡æ¯
            metric_info: æŒ‡æ ‡ä¿¡æ¯
        
        Returns:
            ç½®ä¿¡åº¦è¯„åˆ† (0-1)
        """
        score = 1.0
        
        # 1. æ¥æºå¯ä¿¡åº¦
        source = paper_info.get("source", "unknown")
        source_weight = self.source_weights.get(source, 0.5)
        score *= source_weight
        
        # 2. æŒ‡æ ‡æ•°é‡ï¼ˆæŒ‡æ ‡è¶Šå¤šï¼Œæå–è¶Šå¯é ï¼‰
        metrics_count = len(paper_info.get("normalized_metrics", []))
        if metrics_count > 0:
            score *= min(1.0, 0.5 + metrics_count / 10.0)  # æœ€å¤š 10 ä¸ªæŒ‡æ ‡è¾¾åˆ°æ»¡åˆ†
        
        # 3. ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆæœ‰ä¸Šä¸‹æ–‡è¯´æ˜æå–æ›´å‡†ç¡®ï¼‰
        context = metric_info.get("context", "")
        if len(context) > 50:
            score *= 1.1  # æœ‰ä¸Šä¸‹æ–‡åŠ åˆ†
        score = min(1.0, score)  # é™åˆ¶åœ¨ 1.0
        
        # 4. è½¬æ¢çŠ¶æ€ï¼ˆå¦‚æœç»è¿‡è½¬æ¢ï¼Œå¯èƒ½å¼•å…¥è¯¯å·®ï¼‰
        if metric_info.get("converted", False):
            score *= 0.95  # è½¬æ¢è¿‡çš„æŒ‡æ ‡ç¨å¾®é™æƒ
        
        return score
    
    def find_conflicts(self, normalized_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å†²çªï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«ç½®ä¿¡åº¦åˆ†æï¼‰"""
        print(f"[Verifier] å¼€å§‹éªŒè¯: {len(normalized_results)} ä¸ªç»“æœ")
        
        # æŒ‰æŒ‡æ ‡å’Œæ•°æ®é›†åˆ†ç»„
        metric_groups: Dict[str, List[Dict[str, Any]]] = {}
        
        for result in normalized_results:
            for metric in result.get("normalized_metrics", []):
                metric_name = metric.get("normalized_metric", "")
                value = metric.get("normalized_value", 0)
                
                key = metric_name
                if key not in metric_groups:
                    metric_groups[key] = []
                
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self.calculate_confidence_score(result, metric)
                
                metric_groups[key].append({
                    "paper_id": result.get("paper_id", ""),
                    "title": result.get("title", ""),
                    "source": result.get("source", "unknown"),
                    "metric": metric_name,
                    "value": value,
                    "original_metric": metric.get("original_metric", ""),
                    "context": metric.get("context", ""),
                    "confidence": confidence
                })
        
        # æ£€æµ‹å†²çªï¼ˆè€ƒè™‘ç½®ä¿¡åº¦ï¼‰
        conflicts = []
        for metric_name, values in metric_groups.items():
            if len(values) < 2:
                continue
            
            # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆæŒ‰ç½®ä¿¡åº¦åŠ æƒï¼‰
            weighted_sum = sum(v["value"] * v["confidence"] for v in values)
            confidence_sum = sum(v["confidence"] for v in values)
            weighted_avg = weighted_sum / confidence_sum if confidence_sum > 0 else sum(v["value"] for v in values) / len(values)
            
            # ç®€å•å¹³å‡
            value_list = [v["value"] for v in values]
            avg_value = sum(value_list) / len(value_list)
            max_value = max(value_list)
            min_value = min(value_list)
            diff = max_value - min_value
            
            # è®¡ç®—æ ‡å‡†å·®
            variance = sum((v["value"] - avg_value) ** 2 for v in values) / len(values)
            std_dev = variance ** 0.5
            
            if diff > self.conflict_threshold:
                # æ‰¾å‡ºé«˜ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡åº¦çš„å€¼
                high_conf_values = [v for v in values if v["confidence"] > 0.7]
                low_conf_values = [v for v in values if v["confidence"] < 0.5]
                
                conflicts.append({
                    "metric": metric_name,
                    "papers": values,
                    "avg_value": avg_value,
                    "weighted_avg": weighted_avg,
                    "max_value": max_value,
                    "min_value": min_value,
                    "difference": diff,
                    "std_dev": std_dev,
                    "high_confidence_count": len(high_conf_values),
                    "low_confidence_count": len(low_conf_values),
                    "conflict_level": "high" if diff > 5.0 else "medium" if diff > 2.0 else "low",
                    "recommendation": self._generate_recommendation(values, weighted_avg, diff)
                })
        
        print(f"[Verifier] å‘ç° {len(conflicts)} ä¸ªæ½œåœ¨å†²çª")
        return conflicts
    
    def _generate_recommendation(self, values: List[Dict[str, Any]], weighted_avg: float, diff: float) -> str:
        """ç”Ÿæˆå†²çªè§£å†³å»ºè®®"""
        high_conf = [v for v in values if v.get("confidence", 0) > 0.7]
        
        if len(high_conf) > 0:
            # å¦‚æœæœ‰é«˜ç½®ä¿¡åº¦çš„å€¼ï¼Œæ¨èä½¿ç”¨åŠ æƒå¹³å‡
            return f"å»ºè®®ä½¿ç”¨åŠ æƒå¹³å‡å€¼ {weighted_avg:.2f}%ï¼ˆåŸºäºç½®ä¿¡åº¦ï¼‰ï¼Œå·®å¼‚ {diff:.2f}% å¯èƒ½ç”±äºä¸åŒå®éªŒè®¾ç½®å¯¼è‡´"
        else:
            # å¦‚æœæ²¡æœ‰é«˜ç½®ä¿¡åº¦çš„å€¼ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯
            return f"æ‰€æœ‰å€¼çš„ç½®ä¿¡åº¦éƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹è®ºæ–‡æˆ–ä½¿ç”¨æ›´å¤šæ¥æºéªŒè¯"
    
    async def verify(self, normalized_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯ç»“æœ"""
        conflicts = self.find_conflicts(normalized_results)
        
        # è®¡ç®—ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰
        confidence_scores = []
        for result in normalized_results:
            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—ç½®ä¿¡åº¦
            metric_confidences = []
            for metric in result.get("normalized_metrics", []):
                conf = self.calculate_confidence_score(result, metric)
                metric_confidences.append(conf)
            
            # è®ºæ–‡æ•´ä½“ç½®ä¿¡åº¦ = æŒ‡æ ‡ç½®ä¿¡åº¦çš„å¹³å‡å€¼
            overall_confidence = sum(metric_confidences) / len(metric_confidences) if metric_confidences else 0.5
            
            confidence_scores.append({
                "paper_id": result.get("paper_id", ""),
                "title": result.get("title", ""),
                "source": result.get("source", "unknown"),
                "overall_confidence": overall_confidence,
                "metrics_count": len(metric_confidences),
                "metric_confidences": metric_confidences,
                "confidence_level": "high" if overall_confidence > 0.7 else "medium" if overall_confidence > 0.5 else "low"
            })
        
        result = {
            "total_papers": len(normalized_results),
            "conflicts": conflicts,
            "conflicts_count": len(conflicts),
            "confidence_scores": confidence_scores,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return result


# Pipeline åè°ƒå™¨
class SOTAPipeline:
    """Multi-Agent Pipeline ä¸»åè°ƒå™¨"""
    
    def __init__(self, use_vision: bool = False, vision_model: str = "gpt-4o", use_scholar: bool = False, scholar_timeout: float = 12.0):
        """
        åˆå§‹åŒ– Pipeline
        
        Args:
            use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼ºæå–
            vision_model: Vision Model åç§°
        """
        self.scanner = ScannerAgent(use_scholar=use_scholar, scholar_timeout=scholar_timeout)
        self.extractor = ExtractorAgent(use_vision=use_vision, vision_model=vision_model)
        self.normalizer = NormalizerAgent()
        self.verifier = VerifierAgent()
    
    async def run(self, query: str, max_papers: int = 5) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ Pipeline"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¯åŠ¨ SOTA Pipeline: {query}")
        print(f"{'='*60}\n")
        
        # Step 1: Scanner - æœç´¢è®ºæ–‡
        print("ğŸ“š Step 1: Scanner Agent - æœç´¢è®ºæ–‡...")
        search_results = await self.scanner.search(query, max_results_per_source=max_papers)
        
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡
        all_papers = []
        # ä¼˜å…ˆåˆå¹¶æœ¬åœ°æ¦œå•ä¸ arXivï¼Œå†è¡¥å…… Scholar
        all_papers.extend(search_results.get("leaderboard_results", []))
        all_papers.extend(search_results.get("arxiv_results", []))
        all_papers.extend(search_results.get("google_scholar_results", []))
        
        if not all_papers:
            return {
                "status": "no_results",
                "query": query,
                "message": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡"
            }
        
        # é™åˆ¶å¤„ç†æ•°é‡
        papers_to_process = all_papers[:max_papers]
        print(f"ğŸ“„ å°†å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡ï¼ˆæ¥æºä¼˜å…ˆï¼šleaderboard/arXiv â†’ Scholarï¼‰\n")
        
        # Step 2: Extractor - æå–ä¿¡æ¯
        print("ğŸ” Step 2: Extractor Agent - æå– PDF ä¿¡æ¯...")
        extracted_results = []
        for paper in papers_to_process:
            extracted = await self.extractor.extract(paper)
            if extracted.get("status") == "success":
                extracted_results.append(extracted)
        
        if not extracted_results:
            return {
                "status": "extraction_failed",
                "query": query,
                "message": "PDF æå–å¤±è´¥"
            }
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_results)} ç¯‡è®ºæ–‡\n")
        
        # Step 3: Normalizer - æ ‡å‡†åŒ–
        print("ğŸ“Š Step 3: Normalizer Agent - æ ‡å‡†åŒ–æŒ‡æ ‡...")
        normalized_results = []
        for extracted in extracted_results:
            normalized = await self.normalizer.normalize(extracted)
            normalized_results.append(normalized)
        
        print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ\n")
        
        # Step 4: Verifier - éªŒè¯
        print("ğŸ” Step 4: Verifier Agent - éªŒè¯å’Œå†²çªæ£€æµ‹...")
        verification = await self.verifier.verify(normalized_results)
        
        print(f"âœ… éªŒè¯å®Œæˆ\n")
        
        # æ’åºä¸å»é‡ï¼šå‘å¸ƒæ—¶é—´ä¼˜å…ˆï¼Œå…¶æ¬¡æ¥æºå¯ä¿¡åº¦ï¼Œå…¶æ¬¡ä¸»æŒ‡æ ‡
        def src_weight(src: str) -> int:
            return {"leaderboard": 3, "arxiv": 2, "google_scholar": 1}.get(src, 0)

        def parse_date(s: Optional[str]) -> float:
            if not s:
                return 0.0
            try:
                return datetime.fromisoformat(s).timestamp()
            except Exception:
                return 0.0

        def metric_score(result: Dict[str, Any]) -> float:
            # é€‰å–å¯èƒ½çš„ä¸»æŒ‡æ ‡ï¼ˆä¼˜å…ˆ ao/sr/auc/map/accuracyï¼‰ï¼Œç¼ºçœ 0
            metrics_map = {}
            for m in result.get("normalized_metrics", []):
                metrics_map[m.get("normalized_metric")] = m.get("normalized_value")
            for k in ["ao", "sr", "auc", "map", "accuracy"]:
                v = metrics_map.get(k)
                if isinstance(v, (int, float)):
                    return float(v)
            return 0.0

        # å»é‡ï¼ˆæŒ‰æ ‡é¢˜è§„èŒƒåŒ–ï¼‰
        def norm(s: Optional[str]) -> str:
            return (s or "").strip().lower()

        seen_titles = set()
        deduped_normalized = []
        for r in normalized_results:
            t = norm(r.get("title"))
            if t in seen_titles:
                continue
            seen_titles.add(t)
            deduped_normalized.append(r)

        deduped_normalized.sort(
            key=lambda x: (
                parse_date(x.get("published")),
                src_weight(x.get("source", "")),
                metric_score(x)
            ),
            reverse=True
        )

        # æ±‡æ€»ç»“æœ
        final_result = {
            "status": "success",
            "query": query,
            "pipeline_stages": {
                "scanner": {
                    "total_found": len(all_papers),
                    "processed": len(papers_to_process)
                },
                "extractor": {
                    "successful": len(extracted_results),
                    "failed": len(papers_to_process) - len(extracted_results)
                },
                "normalizer": {
                    "normalized_papers": len(deduped_normalized)
                },
                "verifier": {
                    "conflicts_found": verification.get("conflicts_count", 0)
                }
            },
            "normalized_results": deduped_normalized,
            "verification": verification,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"{'='*60}")
        print(f"âœ… Pipeline å®Œæˆ!")
        print(f"   - å¤„ç†è®ºæ–‡: {len(normalized_results)}")
        print(f"   - å‘ç°å†²çª: {verification.get('conflicts_count', 0)}")
        print(f"{'='*60}\n")
        
        return final_result


# ä¾¿æ·å‡½æ•°
async def run_sota_pipeline(query: str, max_papers: int = 5, use_vision: bool = False, vision_model: str = "gpt-4o", use_scholar: bool = False, scholar_timeout: float = 12.0) -> Dict[str, Any]:
    """
    è¿è¡Œ SOTA Pipeline çš„ä¾¿æ·å‡½æ•°
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_papers: æœ€å¤šå¤„ç†çš„è®ºæ–‡æ•°é‡
        use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼º
        vision_model: Vision Model åç§°
    """
    pipeline = SOTAPipeline(use_vision=use_vision, vision_model=vision_model, use_scholar=use_scholar, scholar_timeout=scholar_timeout)
    return await pipeline.run(query, max_papers)

