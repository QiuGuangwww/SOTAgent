"""
Multi-Agent Pipeline for Trustworthy SOTA Tracking
MVP Version: åŸºç¡€ç‰ˆæœ¬ï¼Œå®ç°æ ¸å¿ƒåŠŸèƒ½

æ¶æ„ï¼š
- Agent A (Scanner): æœç´¢è®ºæ–‡ï¼ˆarXiv + Google Scholarï¼‰
- Agent B (Extractor): ä» PDF æå–æ–‡æœ¬å’Œç®€å•è¡¨æ ¼
- Agent C (Normalizer): æŒ‡æ ‡æ ‡å‡†åŒ–å’Œè½¬æ¢
- Agent D (Verifier): å†²çªæ£€æµ‹å’ŒéªŒè¯
"""

import json
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import asyncio

# Agent A: Scanner
class ScannerAgent:
    """Agent A: è´Ÿè´£æœç´¢è®ºæ–‡ï¼ˆarXiv + Google Scholarï¼‰"""
    
    def __init__(self):
        self.name = "scanner"
    
    async def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ arXiv"""
        try:
            import arxiv
            client = arxiv.Client()
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            results = []
            for paper in client.results(search):
                results.append({
                    "source": "arxiv",
                    "id": paper.get_short_id(),
                    "title": paper.title,
                    "authors": [a.name for a in paper.authors],
                    "summary": paper.summary,
                    "pdf_url": paper.pdf_url,
                    "published": str(paper.published.date()) if paper.published else None,
                    "url": paper.entry_id
                })
            return results
        except Exception as e:
            print(f"[Scanner] arXiv æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_google_scholar(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ Google Scholarï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼Œä½¿ç”¨ scholarly åº“ï¼‰"""
        try:
            from scholarly import scholarly
            search_query = scholarly.search_pubs(query)
            results = []
            
            count = 0
            for pub in search_query:
                if count >= max_results:
                    break
                
                # è·å–è¯¦ç»†ä¿¡æ¯
                try:
                    pub_filled = scholarly.fill(pub)
                    results.append({
                        "source": "google_scholar",
                        "title": pub_filled.get("bib", {}).get("title", ""),
                        "authors": pub_filled.get("bib", {}).get("author", []),
                        "year": pub_filled.get("bib", {}).get("pub_year", ""),
                        "url": pub_filled.get("pub_url", ""),
                        "pdf_url": pub_filled.get("eprint_url", ""),
                        "citations": pub_filled.get("num_citations", 0)
                    })
                    count += 1
                except Exception as e:
                    print(f"[Scanner] è·å– Google Scholar è¯¦æƒ…å¤±è´¥: {e}")
                    continue
                    
            return results
        except Exception as e:
            print(f"[Scanner] Google Scholar æœç´¢å¤±è´¥: {e}")
            print("[Scanner] æç¤º: å¦‚æœ scholarly åº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ Google Scholar æœç´¢")
            return []
    
    async def search(self, query: str, max_results_per_source: int = 10) -> Dict[str, Any]:
        """å¤šæºæœç´¢"""
        print(f"[Scanner] å¼€å§‹æœç´¢: {query}")
        
        # å¹¶è¡Œæœç´¢å¤šä¸ªæ¥æº
        arxiv_results = await self.search_arxiv(query, max_results_per_source)
        scholar_results = await self.search_google_scholar(query, max_results_per_source)
        
        # åˆå¹¶ç»“æœ
        all_results = {
            "query": query,
            "arxiv_results": arxiv_results,
            "google_scholar_results": scholar_results,
            "total_results": len(arxiv_results) + len(scholar_results),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"[Scanner] æ‰¾åˆ° {len(arxiv_results)} ä¸ª arXiv ç»“æœï¼Œ{len(scholar_results)} ä¸ª Google Scholar ç»“æœ")
        return all_results


# Agent B: Extractor
class ExtractorAgent:
    """Agent B: ä» PDF æå–æ–‡æœ¬å’Œç®€å•è¡¨æ ¼ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼‰"""
    
    def __init__(self, use_vision: bool = False, vision_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ– Extractor
        
        Args:
            use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼º
            vision_model: Vision Model åç§°
        """
        self.name = "extractor"
        self.paper_cache_dir = "papers/extracted"
        os.makedirs(self.paper_cache_dir, exist_ok=True)
        
        self.use_vision = use_vision
        self.vision_extractor = None
        
        if use_vision:
            try:
                from vision_extractor import VisionExtractor
                self.vision_extractor = VisionExtractor(vision_model)
                print(f"[Extractor] Vision Model å·²å¯ç”¨: {vision_model}")
            except ImportError:
                print("[Extractor] Vision Extractor ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼")
                self.use_vision = False
    
    def download_pdf(self, pdf_url: str, paper_id: str) -> Optional[str]:
        """ä¸‹è½½ PDF"""
        try:
            import requests
            response = requests.get(pdf_url, timeout=30)
            if response.status_code == 200:
                pdf_path = os.path.join(self.paper_cache_dir, f"{paper_id}.pdf")
                with open(pdf_path, "wb") as f:
                    f.write(response.content)
                return pdf_path
        except Exception as e:
            print(f"[Extractor] ä¸‹è½½ PDF å¤±è´¥ {pdf_url}: {e}")
        return None
    
    def extract_text(self, pdf_path: str) -> str:
        """æå– PDF æ–‡æœ¬ï¼ˆä½¿ç”¨ PyMuPDFï¼‰"""
        try:
            import fitz  # PyMuPDF
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except Exception as e:
            print(f"[Extractor] æ–‡æœ¬æå–å¤±è´¥: {e}")
            # é™çº§åˆ° pdfplumber
            try:
                import pdfplumber
                with pdfplumber.open(pdf_path) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() or ""
                return text
            except Exception as e2:
                print(f"[Extractor] pdfplumber ä¹Ÿå¤±è´¥: {e2}")
                return ""
    
    def extract_tables(self, pdf_path: str) -> List[Dict[str, Any]]:
        """æå–ç®€å•è¡¨æ ¼ï¼ˆä½¿ç”¨ pdfplumberï¼‰"""
        try:
            import pdfplumber
            tables = []
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_tables = page.extract_tables()
                    for table_num, table in enumerate(page_tables):
                        if table and len(table) > 1:  # è‡³å°‘è¦æœ‰è¡¨å¤´å’Œæ•°æ®è¡Œ
                            tables.append({
                                "page": page_num + 1,
                                "table_index": table_num,
                                "data": table,
                                "rows": len(table),
                                "cols": len(table[0]) if table else 0
                            })
            return tables
        except Exception as e:
            print(f"[Extractor] è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def extract_metrics_from_text(self, text: str, context: str = "") -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–æŒ‡æ ‡ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼‰"""
        if self.use_vision and self.vision_extractor:
            # ä½¿ç”¨ LLM è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£
            import asyncio
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            asyncio.run,
                            self.vision_extractor.extract_metrics_with_llm(text, context)
                        )
                        metrics = future.result()
                else:
                    metrics = loop.run_until_complete(
                        self.vision_extractor.extract_metrics_with_llm(text, context)
                    )
                
                if metrics:
                    return metrics
            except Exception as e:
                print(f"[Extractor] Vision æŒ‡æ ‡æå–å¤±è´¥ï¼Œé™çº§åˆ°åŸºç¡€æ¨¡å¼: {e}")
        
        # é™çº§åˆ°åŸºç¡€æ­£åˆ™è¡¨è¾¾å¼æå–
        import re
        metrics = []
        
        # å¸¸è§æŒ‡æ ‡æ¨¡å¼
        metric_patterns = [
            (r"(?:accuracy|acc)\s*[=:]\s*(\d+\.?\d*)\s*%?", "accuracy"),
            (r"(?:f1[- ]?score|f1)\s*[=:]\s*(\d+\.?\d*)\s*%?", "f1_score"),
            (r"(?:mAP|mean average precision)\s*[=:]\s*(\d+\.?\d*)\s*%?", "mAP"),
            (r"(?:top[- ]?1|top1)\s*[=:]\s*(\d+\.?\d*)\s*%?", "top1_accuracy"),
            (r"(?:top[- ]?5|top5)\s*[=:]\s*(\d+\.?\d*)\s*%?", "top5_accuracy"),
        ]
        
        for pattern, metric_name in metric_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                value = float(match.group(1))
                # å¦‚æœæ˜¯ 0-1 èŒƒå›´ï¼Œè½¬æ¢ä¸ºç™¾åˆ†æ¯”
                if value <= 1.0:
                    value = value * 100
                
                metrics.append({
                    "metric": metric_name,
                    "value": value,
                    "unit": "percentage",
                    "context": text[max(0, match.start()-50):match.end()+50]
                })
        
        return metrics
    
    async def extract(self, paper_info: Dict[str, Any]) -> Dict[str, Any]:
        """æå–è®ºæ–‡ä¿¡æ¯"""
        paper_id = paper_info.get("id", "unknown")
        pdf_url = paper_info.get("pdf_url", "")
        
        if not pdf_url:
            return {
                "paper_id": paper_id,
                "status": "no_pdf",
                "error": "æ²¡æœ‰ PDF URL"
            }
        
        print(f"[Extractor] å¼€å§‹æå–: {paper_id}")
        
        # ä¸‹è½½ PDF
        pdf_path = self.download_pdf(pdf_url, paper_id)
        if not pdf_path:
            return {
                "paper_id": paper_id,
                "status": "download_failed",
                "error": "PDF ä¸‹è½½å¤±è´¥"
            }
        
        # æå–æ–‡æœ¬
        text = self.extract_text(pdf_path)
        
        # æå–è¡¨æ ¼
        tables = self.extract_tables(pdf_path)
        
        # å¦‚æœå¯ç”¨ Vision Modelï¼Œå°è¯•å¢å¼ºè¡¨æ ¼æå–
        if self.use_vision and self.vision_extractor:
            try:
                # æ£€æµ‹è¡¨æ ¼ä½ç½®
                table_locations = self.vision_extractor.detect_tables_in_pdf(pdf_path)
                print(f"[Extractor] æ£€æµ‹åˆ° {len(table_locations)} ä¸ªè¡¨æ ¼ä½ç½®")
                
                # å¯¹æ¯ä¸ªè¡¨æ ¼ä½¿ç”¨ Vision Model ç²¾æå–ï¼ˆå¯é€‰ï¼‰
                # è¿™é‡Œå¯ä»¥æ·»åŠ  Vision Model å¤„ç†é€»è¾‘
            except Exception as e:
                print(f"[Extractor] Vision è¡¨æ ¼å¤„ç†å¤±è´¥: {e}")
        
        # ä»æ–‡æœ¬ä¸­æå–æŒ‡æ ‡ï¼ˆæ”¯æŒ Vision Model å¢å¼ºï¼‰
        context = f"Title: {paper_info.get('title', '')}\nSummary: {paper_info.get('summary', '')[:500]}"
        metrics = self.extract_metrics_from_text(text, context)
        
        result = {
            "paper_id": paper_id,
            "title": paper_info.get("title", ""),
            "status": "success",
            "text_length": len(text),
            "tables_count": len(tables),
            "metrics_count": len(metrics),
            "metrics": metrics,
            "tables": tables[:5],  # åªä¿ç•™å‰5ä¸ªè¡¨æ ¼
            "text_preview": text[:1000]  # æ–‡æœ¬é¢„è§ˆ
        }
        
        print(f"[Extractor] æå–å®Œæˆ: {len(metrics)} ä¸ªæŒ‡æ ‡ï¼Œ{len(tables)} ä¸ªè¡¨æ ¼")
        return result


# Agent C: Normalizer
class NormalizerAgent:
    """Agent C: æŒ‡æ ‡æ ‡å‡†åŒ–å’Œè½¬æ¢"""
    
    def __init__(self):
        self.name = "normalizer"
        
        # æŒ‡æ ‡è½¬æ¢è§„åˆ™ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.metric_conversions = {
            "error_rate": lambda x: 100 - x,  # Error Rate -> Accuracy
            "err": lambda x: 100 - x,
            "error": lambda x: 100 - x,
            "classification_error": lambda x: 100 - x,
            "misclassification_rate": lambda x: 100 - x,
            # æ³¨æ„ï¼šF1 å’Œ Accuracy ä¸èƒ½ç›´æ¥è½¬æ¢ï¼Œéœ€è¦ä¸Šä¸‹æ–‡
        }
        
        # æ•°æ®é›†åˆ«åæ˜ å°„ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.dataset_aliases = {
            "imagenet": ["ILSVRC", "ImageNet-1K", "ImageNet", "ImageNet-1k", "ImageNet1K", "ILSVRC2012"],
            "cifar-10": ["CIFAR-10", "CIFAR10", "cifar10", "CIFAR 10"],
            "cifar-100": ["CIFAR-100", "CIFAR100", "cifar100", "CIFAR 100"],
            "got-10k": ["GOT-10k", "GOT10k", "got10k", "GOT-10K"],
            "lasot": ["LaSOT", "LaSOT", "lasot"],
            "trackingnet": ["TrackingNet", "trackingnet", "Tracking Net"],
            "coco": ["COCO", "coco", "MS COCO", "mscoco"],
            "pascal_voc": ["PASCAL VOC", "Pascal VOC", "VOC", "voc"],
            "cityscapes": ["Cityscapes", "cityscapes", "CityScapes"],
        }
        
        # æŒ‡æ ‡ç­‰ä»·å…³ç³»ï¼ˆç”¨äºæ ‡å‡†åŒ–ï¼‰
        self.metric_equivalences = {
            "accuracy": ["acc", "accuracy", "classification accuracy", "top-1 accuracy"],
            "top1_accuracy": ["top-1", "top1", "top 1", "top-1 accuracy", "top1 accuracy"],
            "top5_accuracy": ["top-5", "top5", "top 5", "top-5 accuracy", "top5 accuracy"],
            "f1_score": ["f1", "f1-score", "f1 score", "f1score", "f-measure"],
            "map": ["mAP", "mean average precision", "mean ap", "map"],
            "iou": ["IoU", "iou", "intersection over union", "jaccard index"],
        }
        
        # æŒ‡æ ‡æ ‡å‡†åŒ–åç§°ï¼ˆä½¿ç”¨ç­‰ä»·å…³ç³»ï¼‰
        self.metric_standard_names = {}
        for standard_name, variants in self.metric_equivalences.items():
            self.metric_standard_names[standard_name] = variants
    
    def normalize_metric_name(self, metric_name: str) -> str:
        """æ ‡å‡†åŒ–æŒ‡æ ‡åç§°"""
        metric_lower = metric_name.lower().strip()
        for standard_name, variants in self.metric_standard_names.items():
            if metric_lower in variants:
                return standard_name
        return metric_name.lower()
    
    def normalize_dataset_name(self, dataset_name: str) -> str:
        """æ ‡å‡†åŒ–æ•°æ®é›†åç§°"""
        dataset_lower = dataset_name.lower().strip()
        for standard_name, aliases in self.dataset_aliases.items():
            if dataset_lower in aliases or dataset_lower == standard_name:
                return standard_name
        return dataset_name
    
    def normalize_value(self, value: float, unit: str) -> Tuple[float, str]:
        """æ ‡å‡†åŒ–æ•°å€¼å’Œå•ä½"""
        # ç»Ÿä¸€è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        if unit in ["decimal", "ratio", "fraction"]:
            return value * 100, "percentage"
        elif unit == "percentage":
            return value, "percentage"
        else:
            # é»˜è®¤å‡è®¾æ˜¯ç™¾åˆ†æ¯”
            if 0 <= value <= 1:
                return value * 100, "percentage"
            return value, "percentage"
    
    def convert_metric(self, metric_name: str, value: float) -> Optional[float]:
        """è½¬æ¢æŒ‡æ ‡ï¼ˆå¦‚ Error Rate -> Accuracyï¼‰"""
        metric_lower = metric_name.lower()
        if metric_lower in self.metric_conversions:
            return self.metric_conversions[metric_lower](value)
        return None
    
    async def normalize(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æå–çš„æ•°æ®"""
        print(f"[Normalizer] å¼€å§‹æ ‡å‡†åŒ–: {extracted_data.get('paper_id', 'unknown')}")
        
        normalized_metrics = []
        
        for metric in extracted_data.get("metrics", []):
            metric_name = metric.get("metric", "")
            value = metric.get("value", 0)
            unit = metric.get("unit", "percentage")
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡åç§°
            normalized_name = self.normalize_metric_name(metric_name)
            
            # æ ‡å‡†åŒ–æ•°å€¼
            normalized_value, normalized_unit = self.normalize_value(value, unit)
            
            # å°è¯•è½¬æ¢ï¼ˆå¦‚ Error Rate -> Accuracyï¼‰
            converted_value = self.convert_metric(metric_name, normalized_value)
            if converted_value is not None:
                normalized_metrics.append({
                    "original_metric": metric_name,
                    "normalized_metric": "accuracy",  # Error Rate è½¬æ¢ä¸º Accuracy
                    "original_value": normalized_value,
                    "normalized_value": converted_value,
                    "unit": normalized_unit,
                    "converted": True,
                    "context": metric.get("context", "")
                })
            else:
                normalized_metrics.append({
                    "original_metric": metric_name,
                    "normalized_metric": normalized_name,
                    "original_value": value,
                    "normalized_value": normalized_value,
                    "unit": normalized_unit,
                    "converted": False,
                    "context": metric.get("context", "")
                })
        
        result = {
            "paper_id": extracted_data.get("paper_id", ""),
            "title": extracted_data.get("title", ""),
            "normalized_metrics": normalized_metrics,
            "metrics_count": len(normalized_metrics),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"[Normalizer] æ ‡å‡†åŒ–å®Œæˆ: {len(normalized_metrics)} ä¸ªæŒ‡æ ‡")
        return result


# Agent D: Verifier
class VerifierAgent:
    """Agent D: å†²çªæ£€æµ‹å’ŒéªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self, conflict_threshold: float = 1.0):
        """
        åˆå§‹åŒ– Verifier
        
        Args:
            conflict_threshold: å†²çªé˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œå·®å¼‚è¶…è¿‡æ­¤å€¼è§†ä¸ºå†²çª
        """
        self.name = "verifier"
        self.conflict_threshold = conflict_threshold
        
        # æ¥æºå¯ä¿¡åº¦æƒé‡
        self.source_weights = {
            "arxiv": 1.0,  # arXiv å®˜æ–¹å‘å¸ƒï¼Œå¯ä¿¡åº¦é«˜
            "google_scholar": 0.8,  # Google Scholar èšåˆï¼Œå¯ä¿¡åº¦ä¸­ç­‰
            "paper_pdf": 0.9,  # ç›´æ¥ä»è®ºæ–‡ PDF æå–ï¼Œå¯ä¿¡åº¦é«˜
            "web": 0.6,  # ç½‘é¡µæ¥æºï¼Œå¯ä¿¡åº¦è¾ƒä½
        }
        
        # æ—¶é—´æ–°é²œåº¦æƒé‡ï¼ˆè¶Šæ–°è¶Šå¯ä¿¡ï¼‰
        self.time_decay_factor = 0.1  # æ¯å¹´è¡°å‡ 10%
    
    def calculate_confidence_score(self, paper_info: Dict[str, Any], metric_info: Dict[str, Any]) -> float:
        """
        è®¡ç®—å•ä¸ªæŒ‡æ ‡çš„ç½®ä¿¡åº¦è¯„åˆ†
        
        Args:
            paper_info: è®ºæ–‡ä¿¡æ¯
            metric_info: æŒ‡æ ‡ä¿¡æ¯
        
        Returns:
            ç½®ä¿¡åº¦è¯„åˆ† (0-1)
        """
        score = 1.0
        
        # 1. æ¥æºå¯ä¿¡åº¦
        source = paper_info.get("source", "unknown")
        source_weight = self.source_weights.get(source, 0.5)
        score *= source_weight
        
        # 2. æŒ‡æ ‡æ•°é‡ï¼ˆæŒ‡æ ‡è¶Šå¤šï¼Œæå–è¶Šå¯é ï¼‰
        metrics_count = len(paper_info.get("normalized_metrics", []))
        if metrics_count > 0:
            score *= min(1.0, 0.5 + metrics_count / 10.0)  # æœ€å¤š 10 ä¸ªæŒ‡æ ‡è¾¾åˆ°æ»¡åˆ†
        
        # 3. ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆæœ‰ä¸Šä¸‹æ–‡è¯´æ˜æå–æ›´å‡†ç¡®ï¼‰
        context = metric_info.get("context", "")
        if len(context) > 50:
            score *= 1.1  # æœ‰ä¸Šä¸‹æ–‡åŠ åˆ†
        score = min(1.0, score)  # é™åˆ¶åœ¨ 1.0
        
        # 4. è½¬æ¢çŠ¶æ€ï¼ˆå¦‚æœç»è¿‡è½¬æ¢ï¼Œå¯èƒ½å¼•å…¥è¯¯å·®ï¼‰
        if metric_info.get("converted", False):
            score *= 0.95  # è½¬æ¢è¿‡çš„æŒ‡æ ‡ç¨å¾®é™æƒ
        
        return score
    
    def find_conflicts(self, normalized_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å†²çªï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«ç½®ä¿¡åº¦åˆ†æï¼‰"""
        print(f"[Verifier] å¼€å§‹éªŒè¯: {len(normalized_results)} ä¸ªç»“æœ")
        
        # æŒ‰æŒ‡æ ‡å’Œæ•°æ®é›†åˆ†ç»„
        metric_groups: Dict[str, List[Dict[str, Any]]] = {}
        
        for result in normalized_results:
            for metric in result.get("normalized_metrics", []):
                metric_name = metric.get("normalized_metric", "")
                value = metric.get("normalized_value", 0)
                
                key = metric_name
                if key not in metric_groups:
                    metric_groups[key] = []
                
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self.calculate_confidence_score(result, metric)
                
                metric_groups[key].append({
                    "paper_id": result.get("paper_id", ""),
                    "title": result.get("title", ""),
                    "source": result.get("source", "unknown"),
                    "metric": metric_name,
                    "value": value,
                    "original_metric": metric.get("original_metric", ""),
                    "context": metric.get("context", ""),
                    "confidence": confidence
                })
        
        # æ£€æµ‹å†²çªï¼ˆè€ƒè™‘ç½®ä¿¡åº¦ï¼‰
        conflicts = []
        for metric_name, values in metric_groups.items():
            if len(values) < 2:
                continue
            
            # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆæŒ‰ç½®ä¿¡åº¦åŠ æƒï¼‰
            weighted_sum = sum(v["value"] * v["confidence"] for v in values)
            confidence_sum = sum(v["confidence"] for v in values)
            weighted_avg = weighted_sum / confidence_sum if confidence_sum > 0 else sum(v["value"] for v in values) / len(values)
            
            # ç®€å•å¹³å‡
            value_list = [v["value"] for v in values]
            avg_value = sum(value_list) / len(value_list)
            max_value = max(value_list)
            min_value = min(value_list)
            diff = max_value - min_value
            
            # è®¡ç®—æ ‡å‡†å·®
            variance = sum((v["value"] - avg_value) ** 2 for v in values) / len(values)
            std_dev = variance ** 0.5
            
            if diff > self.conflict_threshold:
                # æ‰¾å‡ºé«˜ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡åº¦çš„å€¼
                high_conf_values = [v for v in values if v["confidence"] > 0.7]
                low_conf_values = [v for v in values if v["confidence"] < 0.5]
                
                conflicts.append({
                    "metric": metric_name,
                    "papers": values,
                    "avg_value": avg_value,
                    "weighted_avg": weighted_avg,
                    "max_value": max_value,
                    "min_value": min_value,
                    "difference": diff,
                    "std_dev": std_dev,
                    "high_confidence_count": len(high_conf_values),
                    "low_confidence_count": len(low_conf_values),
                    "conflict_level": "high" if diff > 5.0 else "medium" if diff > 2.0 else "low",
                    "recommendation": self._generate_recommendation(values, weighted_avg, diff)
                })
        
        print(f"[Verifier] å‘ç° {len(conflicts)} ä¸ªæ½œåœ¨å†²çª")
        return conflicts
    
    def _generate_recommendation(self, values: List[Dict[str, Any]], weighted_avg: float, diff: float) -> str:
        """ç”Ÿæˆå†²çªè§£å†³å»ºè®®"""
        high_conf = [v for v in values if v.get("confidence", 0) > 0.7]
        
        if len(high_conf) > 0:
            # å¦‚æœæœ‰é«˜ç½®ä¿¡åº¦çš„å€¼ï¼Œæ¨èä½¿ç”¨åŠ æƒå¹³å‡
            return f"å»ºè®®ä½¿ç”¨åŠ æƒå¹³å‡å€¼ {weighted_avg:.2f}%ï¼ˆåŸºäºç½®ä¿¡åº¦ï¼‰ï¼Œå·®å¼‚ {diff:.2f}% å¯èƒ½ç”±äºä¸åŒå®éªŒè®¾ç½®å¯¼è‡´"
        else:
            # å¦‚æœæ²¡æœ‰é«˜ç½®ä¿¡åº¦çš„å€¼ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯
            return f"æ‰€æœ‰å€¼çš„ç½®ä¿¡åº¦éƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹è®ºæ–‡æˆ–ä½¿ç”¨æ›´å¤šæ¥æºéªŒè¯"
    
    async def verify(self, normalized_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯ç»“æœ"""
        conflicts = self.find_conflicts(normalized_results)
        
        # è®¡ç®—ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰
        confidence_scores = []
        for result in normalized_results:
            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—ç½®ä¿¡åº¦
            metric_confidences = []
            for metric in result.get("normalized_metrics", []):
                conf = self.calculate_confidence_score(result, metric)
                metric_confidences.append(conf)
            
            # è®ºæ–‡æ•´ä½“ç½®ä¿¡åº¦ = æŒ‡æ ‡ç½®ä¿¡åº¦çš„å¹³å‡å€¼
            overall_confidence = sum(metric_confidences) / len(metric_confidences) if metric_confidences else 0.5
            
            confidence_scores.append({
                "paper_id": result.get("paper_id", ""),
                "title": result.get("title", ""),
                "source": result.get("source", "unknown"),
                "overall_confidence": overall_confidence,
                "metrics_count": len(metric_confidences),
                "metric_confidences": metric_confidences,
                "confidence_level": "high" if overall_confidence > 0.7 else "medium" if overall_confidence > 0.5 else "low"
            })
        
        result = {
            "total_papers": len(normalized_results),
            "conflicts": conflicts,
            "conflicts_count": len(conflicts),
            "confidence_scores": confidence_scores,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return result


# Pipeline åè°ƒå™¨
class SOTAPipeline:
    """Multi-Agent Pipeline ä¸»åè°ƒå™¨"""
    
    def __init__(self, use_vision: bool = False, vision_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ– Pipeline
        
        Args:
            use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼ºæå–
            vision_model: Vision Model åç§°
        """
        self.scanner = ScannerAgent()
        self.extractor = ExtractorAgent(use_vision=use_vision, vision_model=vision_model)
        self.normalizer = NormalizerAgent()
        self.verifier = VerifierAgent()
    
    async def run(self, query: str, max_papers: int = 5) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ Pipeline"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¯åŠ¨ SOTA Pipeline: {query}")
        print(f"{'='*60}\n")
        
        # Step 1: Scanner - æœç´¢è®ºæ–‡
        print("ğŸ“š Step 1: Scanner Agent - æœç´¢è®ºæ–‡...")
        search_results = await self.scanner.search(query, max_results_per_source=max_papers)
        
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡
        all_papers = []
        all_papers.extend(search_results.get("arxiv_results", []))
        all_papers.extend(search_results.get("google_scholar_results", []))
        
        if not all_papers:
            return {
                "status": "no_results",
                "query": query,
                "message": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡"
            }
        
        # é™åˆ¶å¤„ç†æ•°é‡
        papers_to_process = all_papers[:max_papers]
        print(f"ğŸ“„ å°†å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡\n")
        
        # Step 2: Extractor - æå–ä¿¡æ¯
        print("ğŸ” Step 2: Extractor Agent - æå– PDF ä¿¡æ¯...")
        extracted_results = []
        for paper in papers_to_process:
            extracted = await self.extractor.extract(paper)
            if extracted.get("status") == "success":
                extracted_results.append(extracted)
        
        if not extracted_results:
            return {
                "status": "extraction_failed",
                "query": query,
                "message": "PDF æå–å¤±è´¥"
            }
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_results)} ç¯‡è®ºæ–‡\n")
        
        # Step 3: Normalizer - æ ‡å‡†åŒ–
        print("ğŸ“Š Step 3: Normalizer Agent - æ ‡å‡†åŒ–æŒ‡æ ‡...")
        normalized_results = []
        for extracted in extracted_results:
            normalized = await self.normalizer.normalize(extracted)
            normalized_results.append(normalized)
        
        print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ\n")
        
        # Step 4: Verifier - éªŒè¯
        print("ğŸ” Step 4: Verifier Agent - éªŒè¯å’Œå†²çªæ£€æµ‹...")
        verification = await self.verifier.verify(normalized_results)
        
        print(f"âœ… éªŒè¯å®Œæˆ\n")
        
        # æ±‡æ€»ç»“æœ
        final_result = {
            "status": "success",
            "query": query,
            "pipeline_stages": {
                "scanner": {
                    "total_found": len(all_papers),
                    "processed": len(papers_to_process)
                },
                "extractor": {
                    "successful": len(extracted_results),
                    "failed": len(papers_to_process) - len(extracted_results)
                },
                "normalizer": {
                    "normalized_papers": len(normalized_results)
                },
                "verifier": {
                    "conflicts_found": verification.get("conflicts_count", 0)
                }
            },
            "normalized_results": normalized_results,
            "verification": verification,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"{'='*60}")
        print(f"âœ… Pipeline å®Œæˆ!")
        print(f"   - å¤„ç†è®ºæ–‡: {len(normalized_results)}")
        print(f"   - å‘ç°å†²çª: {verification.get('conflicts_count', 0)}")
        print(f"{'='*60}\n")
        
        return final_result


# ä¾¿æ·å‡½æ•°
async def run_sota_pipeline(query: str, max_papers: int = 5, use_vision: bool = False, vision_model: str = "gpt-4o") -> Dict[str, Any]:
    """
    è¿è¡Œ SOTA Pipeline çš„ä¾¿æ·å‡½æ•°
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_papers: æœ€å¤šå¤„ç†çš„è®ºæ–‡æ•°é‡
        use_vision: æ˜¯å¦ä½¿ç”¨ Vision Model å¢å¼º
        vision_model: Vision Model åç§°
    """
    pipeline = SOTAPipeline(use_vision=use_vision, vision_model=vision_model)
    return await pipeline.run(query, max_papers)

